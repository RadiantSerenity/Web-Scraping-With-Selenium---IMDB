{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping imdb.com for the Best Movies and retrieving extra information on the Top 10 as well as the top romance movies\n",
    "\n",
    "1. Initially, to showcase the ability to navigate a website and its individual webpages using Selenium, the project begins at the IMDb homepage. From there, navigation proceeds towards the Top 250 movies webpage. \n",
    "\n",
    "2. Basic information on the Top 250 Movies, as rated by regular IMDb voters, is then retrieved using BeautifulSoup (bs4) and saved into a dataframe. This dataframe is subsequently exported as both a CSV and an XLSX for further analysis.\n",
    "\n",
    "3. To further showcase mastery of Selenium and web scraping, detailed information on the Top 10 movies is gathered. This involves navigating the Selenium Driver to the individual movie webpages and using BeautifulSoup (bs4) to scrape additional information. The more detailed information for the Top 10 is stored in a separate dataframe and exported into a separate CSV and XLSX. Retrieving detailed information on all movies would be too time-consuming for a demonstration such as this. However, the code could be easily adapted to retrieve information on more movies, as the retrieval code is encapsulated within functions. These functions may, unfortunately, need adjustments if IMDb alters the structure of their website.\n",
    "\n",
    "4. Lastly, to further showcase abilities with Selenium, the filter window on IMDb is utilized to retrieve the top romance movies. It is demonstrated that previously written functions can be reused for additional scraping tasks.\n",
    "\n",
    "**Warning! Throughout the project, IMDb has occasionally changed class and ID names of different items. Therefore, functions were written in such a manner where classes and IDs are defined only once. This ensures that if a naming change occurs, only one variable or function needs adaptation. However, this does not guard against larger structural changes to the IMDb website, implying the program requires maintenance and has a limited shelf life.**\n",
    "\n",
    "*Note: By default, the program saves the resulting CSVs and XLSXs to the working directory. Depending on the setup and Integrated Development Environment (IDE), this may be the active directory in which the Jupyter notebook file is located or it may be the base directory. In case the CSVs or XLSXs are not found, please check the base directory* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# We provide information about which is the current working directory on the computer.\n",
    "# In the current working directory the CSV-file that is created through this code will be saved.\n",
    "\n",
    "print(f\"The CSV-files and XLSX-files, that are produced, will be saved in the current working directory.\\nThe following is the current working directory: {os.getcwd()} \\nPlease navigate to the current working directory in order to find the produced CSV-files and XLSX-files.\")\n",
    "print(\"Please check your base directory if the CSV-files and XLSX-files are not in your current working directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and Setting up Selenium Webdriver\n",
    "\n",
    "1. We start by importing all necessary libraries for this code. \n",
    "2. Next, we setup our Selenium driver and set the homepage of IMDb (https://www.imdb.com/?ref_=nv_home) as our starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Set homepage as starting point\n",
    "url = \"https://www.imdb.com/?ref_=nv_home\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating from the Homepage to the Top 250 Movies\n",
    "\n",
    "1. First we start our driver.\n",
    "2. To avoid issues with the driver, we await the cookies popup and click the \"accept button\" once it appears.\n",
    "3. Next, we open up the dropdown menu as a first step to navigate towards the Top 250 movies.\n",
    "4. We await the dropdown menu to load and click the link leading us to the Top 250 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate driver to starting point\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept cookies\n",
    "\n",
    "def accept_cookies(driver, xpath):\n",
    "    # Set up a wait\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Wait for the cookies popup to appear and click the \"Accept\" button\n",
    "    accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "    accept_button.click()\n",
    "\n",
    "# Usage\n",
    "accept_cookies(driver, '//button[@data-testid=\"accept-button\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening up the dropdown menu\n",
    "\n",
    "def click_dropdown_menu(driver, imdb_menu_dropdown_id):\n",
    "    # Set up a wait\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        # Wait until the dropdown menu is clickable\n",
    "        dropdown_menu = wait.until(EC.element_to_be_clickable((By.ID, imdb_menu_dropdown_id)))\n",
    "\n",
    "        # Click the dropdown menu\n",
    "        dropdown_menu.click()\n",
    "    except TimeoutException:\n",
    "        print(\"TimeoutException: Element not found or not clickable within the time limit\")\n",
    "\n",
    "# Usage\n",
    "click_dropdown_menu(driver, 'imdbHeader-navDrawerOpen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigating to the Top 250 Movies Page\n",
    "\n",
    "def navigate_to_top_movies(driver, xpath):\n",
    "    # Set up a wait\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        # Wait until the link is clickable\n",
    "        top_movies_link = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "\n",
    "        # Click the link\n",
    "        top_movies_link.click()\n",
    "    except TimeoutException:\n",
    "        print(\"TimeoutException: Element not found or not clickable within the time limit\")\n",
    "\n",
    "# Usage\n",
    "navigate_to_top_movies(driver, '//a[@href=\"/chart/top/?ref_=nv_mv_250\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the Top 250 Movies\n",
    "\n",
    "1. First, we wait for the webpage of Top 250 Movies to load fully before we proceed.\n",
    "2. We define functions to retrieve the information in a structured way. We use functions to allow for easy future editing and to avoid \"spaghetti coding\".\n",
    "3. We use our functions to retrieve our desired information and save it to a dataframe.\n",
    "4. We print our dataframe to check the content and save it to csv and xlsx for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waiting for things to load\n",
    "\n",
    "# Set up a wait\n",
    "wait = WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions to extract the information on the movies\n",
    "\n",
    "# Function to extract details of a movie made up of sub functions\n",
    "def extract_movie_data(movie, ranking):\n",
    "    # Extract title, year, rating, and link of the movie\n",
    "    title = extract_title(movie)\n",
    "    year = extract_year(movie)\n",
    "    rating = extract_rating(movie)\n",
    "    link = extract_link(movie)\n",
    "    # Return the details as a list\n",
    "    return {'Ranking': ranking, 'Title': title, 'Year': year, 'Rating': rating, 'Link': link}\n",
    "\n",
    "# Function to extract the title of a movie\n",
    "def extract_title(movie):\n",
    "    # Find the title tag\n",
    "    title_tag = movie.find('h3', {'class': 'ipc-title__text'})\n",
    "    # Return the title text, or 'None' if the tag is not found\n",
    "    # Use split function to remove the rank number\n",
    "    return title_tag.text.split('.', 1)[1].strip() if title_tag else 'None'\n",
    "\n",
    "# Function to extract the year of a movie\n",
    "def extract_year(movie):\n",
    "    # Find the year tag\n",
    "    year_tag = movie.find('span', {'class': 'sc-43986a27-8 jHYIIK cli-title-metadata-item'})\n",
    "    # Return the year text, or 'None' if the tag is not found\n",
    "    return year_tag.text if year_tag else 'None'\n",
    "\n",
    "# Function to extract the rating of a movie\n",
    "def extract_rating(movie):\n",
    "    # Find the rating tag\n",
    "    rating_tag = movie.find('span', {'class': 'ipc-rating-star ipc-rating-star--base ipc-rating-star--imdb ratingGroup--imdb-rating'})\n",
    "    # Return the rating text, or 'None' if the tag is not found\n",
    "    return rating_tag.text.strip() if rating_tag else 'None'\n",
    "\n",
    "# Function to extract the link of a movie\n",
    "def extract_link(movie):\n",
    "    # Find the link tag\n",
    "    link_tag = movie.find('a', {'class': 'ipc-title-link-wrapper'})\n",
    "    # Return the full link, or 'None' if the tag is not found\n",
    "    return \"https://www.imdb.com\" + link_tag['href'] if link_tag else 'None'\n",
    "\n",
    "# Function to create a DataFrame from a list of movies\n",
    "def create_movie_dataframe(movies):\n",
    "    # Extract details of each movie and create a list of movies\n",
    "    movie_list = [extract_movie_data(movie, i+1) for i, movie in enumerate(movies)]\n",
    "    # Return a DataFrame created from the list of movies\n",
    "    return pd.DataFrame(movie_list, columns=['Ranking', 'Title', 'Year', 'Rating', 'Link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the information on the movies using our functions\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find all movie list items and use find_all function to retrieve the information\n",
    "# old class tag from 08.12.2023\n",
    "# movies = soup.find_all('li', {'class': 'ipc-metadata-list-summary-item sc-59b6048d-0 cuaJSp cli-parent'})\n",
    "movies = soup.find_all('li', {'class': 'ipc-metadata-list-summary-item sc-3f724978-0 enKyEL cli-parent'})\n",
    "\n",
    "# Create a DataFrame from the list of movies\n",
    "df_top_250_movies = create_movie_dataframe(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_250_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a csv and xlsx file\n",
    "\n",
    "df_top_250_movies.to_csv('top_250_movies.csv', index=False)\n",
    "df_top_250_movies.to_excel('top_250_movies.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving more detailed info on the Top 10 Movies\n",
    "\n",
    "1. We create a copy of our previous dataframe with only the Top 10 rated movies. For these movies, we would like to extract more information from their individual webpages.\n",
    "2. We define our functions that allow us to navigate to the individual movie webpages and to retrieve our desired information.\n",
    "3. We iterate our retrieval over the movies in our Top 10 dataframe\n",
    "4. We print our dataframe to check the content and save it to csv and xlsx for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the top 10 from the top 250 movies\n",
    "\n",
    "df_top_10_movies_detailed = df_top_250_movies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to navigate to a specific movie link\n",
    "def navigate_to_link(driver, link):\n",
    "    # Open the movie link\n",
    "    driver.get(link)\n",
    "    # Wait until the desired element has loaded\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.ipc-chip-list__scroller')))\n",
    "\n",
    "# Function to redirect driver and parse the page content\n",
    "def parse_page(driver):\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Function to extract the description of a movie\n",
    "def extract_description(soup):\n",
    "    # Find the description tag\n",
    "    description_tag = soup.find('span', {'data-testid': 'plot-l'})\n",
    "    # Return the description text, or 'None' if the tag is not found\n",
    "    return description_tag.text if description_tag else 'None'\n",
    "\n",
    "# Function to extract the director of a movie\n",
    "def extract_director(soup):\n",
    "    # Find the director tag\n",
    "    director_tag = soup.find('a', {'href': re.compile(r'/name/nm\\d+/\\?ref_=tt_ov_dr')})\n",
    "    # Return the director text, or 'None' if the tag is not found\n",
    "    return director_tag.text if director_tag else 'None'\n",
    "\n",
    "# Function to extract the writers of a movie\n",
    "def extract_writers(soup):\n",
    "    # Find all writer tags\n",
    "    writer_tags = soup.find_all('a', {'href': re.compile(r'/name/nm\\d+/\\?ref_=tt_ov_wr')})\n",
    "    # Return a string of writers separated by commas, or 'None' if no tags are found\n",
    "    # Use list comprehension to extract the text from each tag\n",
    "    return ', '.join([tag.text for tag in writer_tags]) if writer_tags else 'None'\n",
    "\n",
    "# Function to extract the stars of a movie\n",
    "def extract_stars(soup):\n",
    "    # Find all star tags\n",
    "    star_tags = soup.find_all('a', {'href': re.compile(r'/name/nm\\d+/\\?ref_=tt_ov_st')})\n",
    "    # Return a string of stars separated by commas, or 'None' if no tags are found\n",
    "    # Use list comprehension to extract the text from each tag\n",
    "    return ', '.join([tag.text for tag in star_tags]) if star_tags else 'None'\n",
    "\n",
    "# Function to extract the genres of a movie\n",
    "def extract_genres(soup):\n",
    "    # Find all genre tags\n",
    "    genre_tags = soup.find_all('span', {'class': 'ipc-chip__text'})\n",
    "    # Filter out the 'Back to top' tag and return a string of genres separated by commas, or 'None' if no tags are found\n",
    "    genres = [tag.text for tag in genre_tags if tag.text != 'Back to top']\n",
    "    # Use list comprehension to extract the text from each tag\n",
    "    return ', '.join(genres) if genres else 'None'\n",
    "\n",
    "# Function to extract all the details of a movie\n",
    "def extract_movie_details(link):\n",
    "    # Navigate to the link and parse the page content\n",
    "    navigate_to_link(driver, link)\n",
    "    soup = parse_page(driver)\n",
    "    # Extract the description, director, writers, stars, and genres of the movie\n",
    "    description = extract_description(soup)\n",
    "    director = extract_director(soup)\n",
    "    writers = extract_writers(soup)\n",
    "    stars = extract_stars(soup)\n",
    "    genres = extract_genres(soup)\n",
    "    # Return the extracted data as a dictionary\n",
    "    return {\n",
    "        'Link': link,\n",
    "        'Description': description,\n",
    "        'Director': director,\n",
    "        'Writers': writers,\n",
    "        'Stars': stars,\n",
    "        'Genres': genres\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a list comprehension to create a list of dictionaries containing the extracted data for each Top 10 movie\n",
    "movie_details = [extract_movie_details(link) for link in df_top_10_movies_detailed['Link']]\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_new_info = pd.DataFrame(movie_details)\n",
    "\n",
    "# Drop the 'Link' column from the new DataFrame as not to double the column\n",
    "df_new_info = df_new_info.drop(columns=['Link'])\n",
    "\n",
    "# Concatenate the new information with the existing DataFrame along the columns axis\n",
    "df_top_10_movies_detailed = pd.concat([df_top_10_movies_detailed, df_new_info], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_10_movies_detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the df\n",
    "\n",
    "# Export df_top_10_movies_detailed to a CSV file\n",
    "df_top_10_movies_detailed.to_csv('top_10_movies_detailed.csv', index=False)\n",
    "\n",
    "# Export df_top_10_movies_detailed to an Excel file\n",
    "df_top_10_movies_detailed.to_excel('top_10_movies_detailed.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a filter to receive the top \"Romance\" movies\n",
    "\n",
    "1. To further demonstrate our mastery of selenium, we use filter for \"Romance\" movies. We move back to the Top 250 webpage\n",
    "2. We access the filter window by clicking the filter button\n",
    "3. We click the \"Romance\" filter\n",
    "4. We close the filter window\n",
    "5. We scrape the data using our previous functions\n",
    "6. We check and save the data\n",
    "7. We close our selenium driver and end our scraping exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move back to top 250 movies page\n",
    "\n",
    "navigate_to_link(driver, \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the filter button\n",
    "\n",
    "def click_filter_button(driver, css_selector):\n",
    "    # Set up a wait\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        # Wait until the filter button is clickable\n",
    "        filter_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "        # Click the filter button\n",
    "        filter_button.click()\n",
    "    except TimeoutException:\n",
    "        print(\"TimeoutException: Element not found or not clickable within the time limit\")\n",
    "\n",
    "# Usage\n",
    "click_filter_button(driver, 'button[data-testid=\"filter-menu-button\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the filter for \"Romance\"\n",
    "\n",
    "def click_genre_button(driver, css_selector):\n",
    "    # Set up a wait\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        # Wait until the genre button is clickable\n",
    "        genre_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "        # Click the genre button\n",
    "        genre_button.click()\n",
    "    except TimeoutException:\n",
    "        print(\"TimeoutException: Element not found or not clickable within the time limit\")\n",
    "\n",
    "# Usage\n",
    "click_genre_button(driver, 'button[data-testid=\"filter-genre-chip-Romance\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close filter window\n",
    "\n",
    "def close_filter_window(driver, css_selector):\n",
    "    # Set up a wait\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        # Wait until the close button is clickable\n",
    "        close_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "        # Click the close button\n",
    "        close_button.click()\n",
    "    except TimeoutException:\n",
    "        print(\"TimeoutException: Element not found or not clickable within the time limit\")\n",
    "\n",
    "# Usage\n",
    "close_filter_window(driver, 'button[aria-label=\"Close Prompt\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waiting for things to load\n",
    "\n",
    "# Set up a wait\n",
    "wait = WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the information on the romance movies using our functions\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find all movie list items and use find_all function to retrieve the information\n",
    "movies = soup.find_all('li', {'class': 'ipc-metadata-list-summary-item sc-3f724978-0 enKyEL cli-parent'})\n",
    "\n",
    "# Create a DataFrame from the list of movies\n",
    "df_top_romance_movies = create_movie_dataframe(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_romance_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the df\n",
    "\n",
    "# Export df_top_10_movies_detailed to a CSV file\n",
    "df_top_romance_movies.to_csv('top_romance_movies.csv', index=False)\n",
    "\n",
    "# Export df_top_10_movies_detailed to an Excel file\n",
    "df_top_romance_movies.to_excel('top_romance_movies.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the driver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
